{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import gzip\n",
    "import pickle\n",
    "from random import sample\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('../data/idx_files/regions_idx.pickle.gz', 'rb') as f:\n",
    "    regions = pickle.load(f) #1817918\n",
    "    \n",
    "with gzip.open('../data/idx_files/samples_idx.pickle.gz', 'rb') as f:\n",
    "    samples = pickle.load(f) #52\n",
    "    \n",
    "with gzip.open('../data/idx_files/tfs_idx.pickle.gz', 'rb') as f:\n",
    "    tfs = pickle.load(f) #163\n",
    "    \n",
    "tfs = pd.Series(tfs).sort_values()\n",
    "regions = pd.Series(regions).sort_values()\n",
    "\n",
    "data = np.load(\"../data/matrices/matrix2d.ReMap+UniBind.full.npz\")\n",
    "\n",
    "for i in data.files:\n",
    "    matrix2d_full = data[i] #(1817918, 163)\n",
    "    \n",
    "df = pd.DataFrame(data=matrix2d_full, index=regions.index, columns=tfs.index)\n",
    "df_transposed = df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_transposed\n",
    "X = X[(X.T != 0).any()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()\n",
    "\n",
    "#embedding = reducer.fit_transform(df_transposed.to_numpy())\n",
    "embedding = reducer.fit_transform(X.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embedding = pd.DataFrame(data=embedding, index=X.index, columns=[\"x\",\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(umap_embedding, x=\"x\", y=\"y\", #color=\"Labels\",\n",
    "                text=umap_embedding.index)\n",
    "\n",
    "fig.update_traces(marker=dict(size=12,\n",
    "                              line=dict(width=2,\n",
    "                                        color='DarkSlateGrey')),\n",
    "                  selector=dict(mode='markers'))\n",
    "\n",
    "fig.update_layout(title_text='', \n",
    "                  xaxis_title='',\n",
    "                  yaxis_title='',\n",
    "                  plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)')\n",
    "fig.update_xaxes(showline=True, linewidth=2, linecolor='black')\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='black')\n",
    "\n",
    "fig.update_layout({'width':800, 'height':800,\n",
    "                         'showlegend':False\n",
    "                         })\n",
    "\n",
    "layout = go.Layout(\n",
    "   title = \"\",\n",
    "   xaxis = dict(\n",
    "      title = 'UMAP1',\n",
    "      titlefont = dict(\n",
    "         family = 'Arial',\n",
    "         size = 18,\n",
    "         color = 'black'\n",
    "      )     \n",
    "   ),\n",
    "   yaxis = dict(\n",
    "      title = 'UMAP2',\n",
    "      titlefont = dict(\n",
    "         family = 'Arial',\n",
    "         size = 18,\n",
    "         color = 'black'\n",
    "      )\n",
    "   )\n",
    ")\n",
    "\n",
    "fig.update_layout(layout)\n",
    "\n",
    "fig.update_traces(textposition=\"bottom center\")\n",
    "fig.update_layout(uniformtext_minsize=8)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap of the TF-TF similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"../data/matrices/matrix2d.ReMap+UniBind.partial.npz\")\n",
    "\n",
    "for i in data.files:\n",
    "    matrix2d_partial = data[i] #(1817918, 163)\n",
    "    \n",
    "df_partial = pd.DataFrame(data=matrix2d_partial, index=regions.index, columns=tfs.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_classes = {}\n",
    "\n",
    "with open(\"../data/clusters.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        \n",
    "        line_parts = line.strip().split()\n",
    "        tf_class = line_parts[-1]\n",
    "        tf_name = line_parts[0]\n",
    "\n",
    "        tf_classes[tf_name.upper()] = tf_class\n",
    "    \n",
    "tf_classes = pd.Series(tf_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_classes = tf_classes.sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_del = np.array(list(df_partial))[~np.isin(list(df_partial), tf_classes.index)]\n",
    "df_partial = df_partial.drop(col_del, axis=1)\n",
    "\n",
    "tf_classes_df = tf_classes[list(df_partial)].sort_values(ascending=True)\n",
    "df_partial = df_partial[tf_classes_df.index]\n",
    "\n",
    "df_partial_transposed = df_partial.T\n",
    "df_partial_transposed = df_partial_transposed[(df_partial_transposed.fillna(0).T != 0).any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "def cosine_without_nones(u, v):\n",
    "    \n",
    "    dataframe = pd.DataFrame({0:u,1:v})\n",
    "    \n",
    "    return cosine_similarity(dataframe.T.dropna(axis=1))[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = squareform(pdist(df_partial_transposed.to_numpy(), cosine_without_nones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(dm.shape[0]):\n",
    "    dm[i,i] = 1\n",
    "    \n",
    "tf_classes_df = tf_classes_df.drop(labels=[\"SMAD3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tf_cosine_matrix.pickle', 'wb') as f:\n",
    "    pickle.dump(dm, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=go.Heatmap(\n",
    "                   z=dm,\n",
    "                   x=df_partial_transposed.index,\n",
    "                   y=df_partial_transposed.index,\n",
    "                   hoverongaps = False\n",
    "    ))\n",
    "\n",
    "fig.update_layout(title='',\n",
    "                 font=dict(\n",
    "                     family=\"Arial\",\n",
    "                     size=14,\n",
    "                     color=\"black\"\n",
    "                 ))\n",
    "\n",
    "\n",
    "layout = go.Layout(\n",
    "   title = \"\",\n",
    "   xaxis = dict(\n",
    "      title = '',\n",
    "      tickfont = dict(\n",
    "         family = 'Arial',\n",
    "         size = 3,\n",
    "         color = 'black'\n",
    "      )     \n",
    "   ),\n",
    "   yaxis = dict(\n",
    "      title = '',\n",
    "      tickfont = dict(\n",
    "         family = 'Arial',\n",
    "         size = 3,\n",
    "         color = 'black'\n",
    "      )\n",
    "   )\n",
    ")\n",
    "\n",
    "fig.update_layout(layout)\n",
    "\n",
    "fig.update_layout(autosize=False,width=800,height=800)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the TFs for the TL experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for TFs with the same BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"../data/matrices/matrix2d.ReMap+UniBind.sparse.npz\")\n",
    "\n",
    "for i in data.files:\n",
    "    matrix2d_sparse = data[i] #(1817918, 163)\n",
    "    \n",
    "df_sparse = pd.DataFrame(data=matrix2d_sparse, index=regions.index, columns=tfs.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sparse = df_sparse.loc[:, (df_sparse.fillna(0) != 0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the TF - Jaspar cluster relations\n",
    "tf_clust_corr = pd.read_csv(\"../data/TF_clust_correspond.tsv\", sep=\"\\t\", header=None)\n",
    "tf_clust_corr = pd.Series(tf_clust_corr[1].values, index = tf_clust_corr[0].values) \n",
    "\n",
    "clusters_multi_modes = {}\n",
    "for tf in list(df_sparse): \n",
    "    clusts = tf_clust_corr[tf]\n",
    "    clusts = clusts.split(\",\")\n",
    "    for c in clusts:\n",
    "        if c not in clusters_multi_modes.keys():\n",
    "            clusters_multi_modes[c] = []\n",
    "        clusters_multi_modes[c].append(tf)    \n",
    "    \n",
    "clusters_multi_modes = pd.Series(clusters_multi_modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_sum(tf):\n",
    "    return df_sparse[tf].sum()\n",
    "\n",
    "clusters_multi_modes_sorted = clusters_multi_modes.apply(lambda x: sorted(x, key=tf_sum,\n",
    "                                                                         reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bms = clusters_multi_modes_sorted.apply(lambda x: len(x))\n",
    "bms = list(bms.sort_values(ascending=False).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_classes = {}\n",
    "\n",
    "with open(\"../data/clusters.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        \n",
    "        line_parts = line.strip().split()\n",
    "        tf_class = line_parts[-1]\n",
    "        tf_name = line_parts[0]\n",
    "\n",
    "        tf_classes[tf_name.upper()] = tf_class\n",
    "    \n",
    "tf_classes = pd.Series(tf_classes)\n",
    "\n",
    "tf_classes = tf_classes.sort_values(ascending=True)\n",
    "tf_classes_df = tf_classes[list(df_sparse)].sort_values(ascending=True)\n",
    "\n",
    "tf_classes = tf_classes_df.apply(lambda x: \".\".join(x.split(\".\")[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfs = {}#set()\n",
    "\n",
    "for bm in bms:\n",
    "    if len(tfs.keys()) == 0:\n",
    "        tfs[bm] = clusters_multi_modes_sorted[bm][0]\n",
    "    else:\n",
    "        skip = False\n",
    "        for tf in tfs.values():\n",
    "            if tf_classes[clusters_multi_modes_sorted[bm][0]] == tf_classes[tf]:\n",
    "                skip = True\n",
    "                break\n",
    "        if not skip:\n",
    "            tfs[bm] = clusters_multi_modes_sorted[bm][0]\n",
    "            \n",
    "tfs = pd.Series(tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/clusters_multi_modes_sorted.pickle', 'wb') as f:\n",
    "    pickle.dump(clusters_multi_modes_sorted, f)\n",
    "    \n",
    "with open('../data/tf_clust_corr.pickle', 'wb') as f:\n",
    "    pickle.dump(tf_clust_corr[list(df_sparse)], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for cofactors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFs_to_analyze = ['MAX', 'JUND', 'SPI1', 'SP1', 'HNF4A', 'EGR1']\n",
    "#dm\n",
    "tfs = {}\n",
    "#for tf in TFs_to_analyze:\n",
    "for tf in df_partial_transposed.index:\n",
    "    tfs[tf] = np.where(df_partial_transposed.index == tf)[0][0]\n",
    "tfs = pd.Series(tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tf_clust_corr.pickle', 'rb') as f:\n",
    "    tf_clust_corr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cofactors = {}\n",
    "for tf in tfs.index:\n",
    "    test = pd.Series(dm[tfs[tf],:], index=df_partial_transposed.index)\n",
    "    test = test.sort_values(ascending=False)\n",
    "    test2 = tf_clust_corr.apply(lambda x: x.split(\",\"))\n",
    "    test = test[test2.apply(lambda x: ~np.any(np.isin(x, test2[tf])))].sort_values(ascending=False)\n",
    "    cofactors[tf] = list(test[:5].index)\n",
    "    #cofactors[tf] = list(test[:10].index)\n",
    "cofactors = pd.Series(cofactors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/cofactors.pickle', 'wb') as f:\n",
    "    pickle.dump(cofactors, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for BM partners with the smallest correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/clusters_multi_modes_sorted.pickle', 'rb') as f:\n",
    "    clusters_multi_modes_sorted = pickle.load(f)\n",
    "    \n",
    "with open('../data/tf_clust_corr.pickle', 'rb') as f:\n",
    "    tf_clust_corr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binding_modes = {\"MAX\":\"7\", \"JUND\":\"1\", \"SPI1\":\"16\", \"SP1\":\"34\", \"EGR1\":\"34\", \"HNF4A\":\"4\"}\n",
    "TFs_to_analyze = [\"MAX\", \"JUND\", \"SPI1\", \"SP1\", \"EGR1\", \"HNF4A\", \"FOXA1\", \"SOX6\", \"TBP\"]\n",
    "tfs = {}\n",
    "#for tf in TFs_to_analyze:\n",
    "for tf in TFs_to_analyze:\n",
    "    tfs[tf] = np.where(df_partial_transposed.index == tf)[0][0]\n",
    "tfs = pd.Series(tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_cor_bms = {}\n",
    "for tf in tfs.index:\n",
    "    test = pd.Series(dm[tfs[tf],:], index=df_partial_transposed.index)    \n",
    "    not_cor_bms[tf] = list(test[clusters_multi_modes_sorted[binding_modes[tf]]].sort_values(ascending=False)[-5:].index)\n",
    "not_cor_bms = pd.Series(not_cor_bms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/not_cor_bms.pickle', 'wb') as f:\n",
    "    pickle.dump(not_cor_bms, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for binding partners from STRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load string data\n",
    "TFs_to_analyze = ['MAX', 'JUND', 'SPI1', 'SP1', 'HNF4A', 'EGR1']\n",
    "\n",
    "string_data = pd.read_csv(\"../data/string.sorted.tsv\", sep=\"\\t\", header=None)\n",
    "\n",
    "tf_clust_corr = pd.read_csv(\"../data/TF_clust_correspond.tsv\", sep=\"\\t\", header=None)\n",
    "tf_clust_corr = pd.Series(tf_clust_corr[1].values, index = tf_clust_corr[0].values)\n",
    "\n",
    "string_partners = {}\n",
    "for tf in TFs_to_analyze:\n",
    "    TF_data = string_data[string_data[0] == tf]\n",
    "    TF_binding_buds = TF_data[1]\n",
    "    TF_binding_buds = TF_binding_buds[np.isin(TF_binding_buds, df_partial_transposed.index)]\n",
    "    \n",
    "    test2 = tf_clust_corr.apply(lambda x: x.split(\",\"))\n",
    "    test2 = test2.apply(lambda x: ~np.any(np.isin(x, test2[tf])))\n",
    "    test2 = test2[test2 == True]\n",
    "    TF_binding_buds = TF_binding_buds[np.isin(TF_binding_buds.values, test2.index)]\n",
    "    string_partners[tf]= list(TF_binding_buds.values)[:5]\n",
    "    \n",
    "string_partners = pd.Series(string_partners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/string_partners.pickle', 'wb') as f:\n",
    "    pickle.dump(string_partners, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for best correlated TFs (doesn't matter if BM or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/clusters_multi_modes_sorted.pickle', 'rb') as f:\n",
    "    clusters_multi_modes_sorted = pickle.load(f)\n",
    "    \n",
    "with open('../data/tf_clust_corr.pickle', 'rb') as f:\n",
    "    tf_clust_corr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFs_to_analyze = [\"MAX\", \"JUND\", \"SPI1\", \"SP1\", \"HNF4A\"]\n",
    "tfs = {}\n",
    "#for tf in TFs_to_analyze:\n",
    "for tf in TFs_to_analyze: \n",
    "    tfs[tf] = np.where(df_partial_transposed.index == tf)[0][0]\n",
    "tfs = pd.Series(tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cor_tfs = {}\n",
    "\n",
    "for tf in tfs.index:\n",
    "    test = pd.Series(dm[tfs[tf],:], index=df_partial_transposed.index)\n",
    "    test = test.sort_values(ascending=False)[1:6]\n",
    "    \n",
    "    best_cor_tfs[tf] = list(test.index)\n",
    "    \n",
    "best_cor_tfs = pd.Series(best_cor_tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/best_cor_tfs.pickle', 'wb') as f:\n",
    "    pickle.dump(best_cor_tfs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for STRING TFs (doesn't matter if BM or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/clusters_multi_modes_sorted.pickle', 'rb') as f:\n",
    "    clusters_multi_modes_sorted = pickle.load(f)\n",
    "    \n",
    "with open('../data/tf_clust_corr.pickle', 'rb') as f:\n",
    "    tf_clust_corr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load string data\n",
    "TFs_to_analyze = ['MAX', 'JUND', 'SPI1', 'SP1', 'HNF4A']\n",
    "\n",
    "string_data = pd.read_csv(\"../data/string.sorted.tsv\", sep=\"\\t\", header=None)\n",
    "\n",
    "tf_clust_corr = pd.read_csv(\"../data/TF_clust_correspond.tsv\", sep=\"\\t\", header=None)\n",
    "tf_clust_corr = pd.Series(tf_clust_corr[1].values, index = tf_clust_corr[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_partners = {}\n",
    "for tf in TFs_to_analyze:\n",
    "    TF_data = string_data[string_data[0] == tf]\n",
    "    TF_binding_buds = TF_data[1]\n",
    "    TF_binding_buds = TF_binding_buds[np.isin(TF_binding_buds, df_partial_transposed.index)]\n",
    "    \n",
    "    string_partners[tf]= list(TF_binding_buds.values)[:5]\n",
    "    \n",
    "string_partners = pd.Series(string_partners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/string_partners_best.pickle', 'wb') as f:\n",
    "    pickle.dump(string_partners, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
